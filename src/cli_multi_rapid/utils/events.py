"""Event emission utilities for workflow progress tracking."""import jsonimport timeimport pathlibfrom datetime import datetimefrom typing import Any, Dict, Optionaldef emit_event(    event_type: str,    payload: Dict[str, Any],    out_dir: str = "artifacts/_events") -> str:    """Emit a structured event to the events directory."""    p = pathlib.Path(out_dir)    p.mkdir(parents=True, exist_ok=True)    ts = time.strftime('%Y%m%dT%H%M%S')    event_file = p / f'{ts}-{event_type}.json'    event_data = {        'type': event_type,        'timestamp': datetime.now().isoformat(),        **payload    }    event_file.write_text(        json.dumps(event_data, ensure_ascii=False, indent=2),        encoding='utf-8'    )    return str(event_file)def emit_progress_event(    job_id: str,    phase: str,    status: str,    message: str = "",    percent: float = 0.0,    workstream_id: Optional[str] = None,    status_reason: Optional[str] = None,    result_uri: str = "",    metrics: Optional[Dict[str, Any]] = None,    out_dir: str = "artifacts/_events") -> str:    """Emit a progress event conforming to ProgressEvent schema."""    if metrics is None:        metrics = {}    payload = {        "job_id": job_id,        "workstream_id": workstream_id,        "phase": phase,        "status": status,        "status_reason": status_reason,        "percent": percent,        "message": message,        "result_uri": result_uri,        "timestamp": datetime.now().isoformat(),        "metrics": metrics    }    return emit_event("progress", payload, out_dir)def emit_node_event(    job_id: str,    node_id: str,    status: str,    message: str = "",    tool_used: Optional[str] = None,    execution_time: float = 0.0,    artifacts: Optional[list] = None,    out_dir: str = "artifacts/_events") -> str:    """Emit a node execution event."""    if artifacts is None:        artifacts = []    payload = {        "job_id": job_id,        "node_id": node_id,        "status": status,        "message": message,        "tool_used": tool_used,        "execution_time": execution_time,        "artifacts": artifacts    }    return emit_event("node", payload, out_dir)def read_events(    job_id: Optional[str] = None,    event_type: Optional[str] = None,    events_dir: str = "artifacts/_events") -> list:    """Read events from the events directory, optionally filtered."""    p = pathlib.Path(events_dir)    if not p.exists():        return []    events = []    for event_file in p.glob("*.json"):        try:            with open(event_file, 'r', encoding='utf-8') as f:                event_data = json.load(f)            # Apply filters            if job_id and event_data.get("job_id") != job_id:                continue            if event_type and event_data.get("type") != event_type:                continue            events.append(event_data)        except Exception:            # Skip malformed event files            continue    # Sort by timestamp    events.sort(key=lambda x: x.get("timestamp", ""))    return eventsdef cleanup_old_events(    days_to_keep: int = 7,    events_dir: str = "artifacts/_events") -> int:    """Clean up old event files, keeping only recent ones."""    p = pathlib.Path(events_dir)    if not p.exists():        return 0    cutoff_time = time.time() - (days_to_keep * 24 * 60 * 60)    removed_count = 0    for event_file in p.glob("*.json"):        try:            if event_file.stat().st_mtime < cutoff_time:                event_file.unlink()                removed_count += 1        except Exception:            pass  # Ignore cleanup errors    return removed_count