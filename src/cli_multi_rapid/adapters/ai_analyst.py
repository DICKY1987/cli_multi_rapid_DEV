#!/usr/bin/env python3"""AI Analyst AdapterProvides AI-powered code analysis, review, and planning capabilities.Focuses on understanding code patterns, generating insights, and planning changes."""import jsonimport loggingfrom pathlib import Pathfrom typing import Any, Optionalfrom .base_adapter import AdapterResult, AdapterType, BaseAdapterlogger = logging.getLogger(__name__)class AIAnalystAdapter(BaseAdapter):    """AI-powered code analysis and planning adapter."""    def __init__(self):        super().__init__(            name="ai_analyst",            adapter_type=AdapterType.AI,            description="AI-powered code analysis, review, and planning",        )    def execute(        self,        step: dict[str, Any],        context: Optional[dict[str, Any]] = None,        files: Optional[str] = None,    ) -> AdapterResult:        """Execute AI analysis workflow step."""        self._log_execution_start(step)        try:            # Extract parameters            with_params = self._extract_with_params(step)            emit_paths = self._extract_emit_paths(step)            # Get analysis type and parameters            analysis_type = with_params.get("analysis_type", "code_review")            focus_areas = with_params.get(                "focus_areas", ["quality", "bugs", "performance"]            )            detail_level = with_params.get("detail_level", "medium")            model = with_params.get("model", "claude-3-5-sonnet-20241022")            # Execute analysis based on type            if analysis_type == "code_review":                result = self._execute_code_review(                    files=files,                    focus_areas=focus_areas,                    detail_level=detail_level,                    emit_paths=emit_paths,                    model=model,                )            elif analysis_type == "architecture_analysis":                result = self._execute_architecture_analysis(                    files=files,                    detail_level=detail_level,                    emit_paths=emit_paths,                    model=model,                )            elif analysis_type == "refactor_planning":                result = self._execute_refactor_planning(                    files=files,                    focus_areas=focus_areas,                    emit_paths=emit_paths,                    model=model,                )            elif analysis_type == "test_planning":                result = self._execute_test_planning(                    files=files,                    detail_level=detail_level,                    emit_paths=emit_paths,                    model=model,                )            else:                return AdapterResult(                    success=False,                    error=f"Unsupported analysis type: {analysis_type}",                )            self._log_execution_complete(result)            return result        except Exception as e:            error_msg = f"AI analysis failed: {str(e)}"            logger.error(error_msg)            return AdapterResult(success=False, error=error_msg)    def _execute_code_review(        self,        files: Optional[str],        focus_areas: list[str],        detail_level: str,        emit_paths: list[str],        model: str,    ) -> AdapterResult:        """Execute AI-powered code review."""        # Simulate AI code review (placeholder - would integrate with actual AI APIs)        file_list = self._resolve_file_pattern(files) if files else []        if not file_list:            return AdapterResult(                success=False,                error="No files found for analysis",            )        # Generate mock analysis results        analysis_results = {            "review_type": "code_review",            "model_used": model,            "files_analyzed": file_list,            "focus_areas": focus_areas,            "detail_level": detail_level,            "findings": self._generate_mock_code_review_findings(                file_list, focus_areas            ),            "summary": {                "total_files": len(file_list),                "issues_found": 5,  # Mock value                "critical_issues": 1,                "recommendations": 8,            },            "timestamp": self._get_timestamp(),        }        # Save analysis to artifacts        artifacts = self._save_analysis_artifacts(emit_paths, analysis_results)        return AdapterResult(            success=True,            tokens_used=self._estimate_tokens_for_analysis(file_list, detail_level),            artifacts=artifacts,            output="Code review completed successfully",            metadata={                "analysis_type": "code_review",                "files_analyzed": len(file_list),                "issues_found": analysis_results["summary"]["issues_found"],            },        )    def _execute_architecture_analysis(        self,        files: Optional[str],        detail_level: str,        emit_paths: list[str],        model: str,    ) -> AdapterResult:        """Execute architecture analysis."""        file_list = self._resolve_file_pattern(files) if files else []        analysis_results = {            "analysis_type": "architecture_analysis",            "model_used": model,            "files_analyzed": file_list,            "detail_level": detail_level,            "architecture_insights": {                "patterns_detected": ["MVC", "Adapter Pattern", "Factory Pattern"],                "coupling_analysis": "Medium coupling detected between modules",                "cohesion_analysis": "Good cohesion within individual classes",                "dependency_graph": self._generate_mock_dependency_analysis(file_list),            },            "recommendations": [                "Consider extracting common interfaces for better testability",                "Reduce coupling between adapter classes and core logic",                "Add more comprehensive error handling patterns",            ],            "timestamp": self._get_timestamp(),        }        artifacts = self._save_analysis_artifacts(emit_paths, analysis_results)        return AdapterResult(            success=True,            tokens_used=self._estimate_tokens_for_analysis(file_list, detail_level)            * 1.5,            artifacts=artifacts,            output="Architecture analysis completed",            metadata={                "analysis_type": "architecture_analysis",                "patterns_found": len(                    analysis_results["architecture_insights"]["patterns_detected"]                ),            },        )    def _execute_refactor_planning(        self,        files: Optional[str],        focus_areas: list[str],        emit_paths: list[str],        model: str,    ) -> AdapterResult:        """Execute refactoring planning."""        file_list = self._resolve_file_pattern(files) if files else []        refactor_plan = {            "plan_type": "refactor_planning",            "model_used": model,            "files_analyzed": file_list,            "focus_areas": focus_areas,            "refactor_opportunities": [                {                    "type": "extract_method",                    "file": "src/cli_multi_rapid/adapters/base_adapter.py",                    "location": "BaseAdapter.__init__",                    "reason": "Method too long, extract logging setup",                    "priority": "medium",                },                {                    "type": "introduce_interface",                    "files": file_list,                    "reason": "Common patterns across adapters can be abstracted",                    "priority": "high",                },            ],            "execution_plan": {                "phases": [                    {                        "phase": 1,                        "description": "Extract common logging patterns",                        "files": ["base_adapter.py"],                        "estimated_effort": "2 hours",                    },                    {                        "phase": 2,                        "description": "Introduce adapter interfaces",                        "files": file_list,                        "estimated_effort": "4 hours",                    },                ],                "total_estimated_effort": "6 hours",            },            "timestamp": self._get_timestamp(),        }        artifacts = self._save_analysis_artifacts(emit_paths, refactor_plan)        return AdapterResult(            success=True,            tokens_used=self._estimate_tokens_for_analysis(file_list, "high") * 2,            artifacts=artifacts,            output="Refactor planning completed",            metadata={                "plan_type": "refactor_planning",                "opportunities_found": len(refactor_plan["refactor_opportunities"]),                "phases": len(refactor_plan["execution_plan"]["phases"]),            },        )    def _execute_test_planning(        self,        files: Optional[str],        detail_level: str,        emit_paths: list[str],        model: str,    ) -> AdapterResult:        """Execute test planning analysis."""        file_list = self._resolve_file_pattern(files) if files else []        test_plan = {            "plan_type": "test_planning",            "model_used": model,            "files_analyzed": file_list,            "detail_level": detail_level,            "coverage_analysis": {                "current_coverage": "75%",  # Mock data                "uncovered_areas": [                    "Error handling in adapter base classes",                    "Edge cases in file pattern resolution",                    "Integration scenarios between adapters",                ],            },            "recommended_tests": [                {                    "type": "unit_test",                    "target": "AIEditorAdapter.estimate_cost",                    "description": "Test token estimation accuracy",                    "priority": "high",                },                {                    "type": "integration_test",                    "target": "AI adapter workflow execution",                    "description": "End-to-end AI editing workflow",                    "priority": "medium",                },            ],            "test_strategy": {                "unit_tests": "Focus on individual adapter methods",                "integration_tests": "Test adapter interactions with router",                "e2e_tests": "Full workflow validation with mocked AI responses",            },            "timestamp": self._get_timestamp(),        }        artifacts = self._save_analysis_artifacts(emit_paths, test_plan)        return AdapterResult(            success=True,            tokens_used=self._estimate_tokens_for_analysis(file_list, detail_level),            artifacts=artifacts,            output="Test planning completed",            metadata={                "plan_type": "test_planning",                "recommended_tests": len(test_plan["recommended_tests"]),            },        )    def _generate_mock_code_review_findings(        self,        files: list[str],        focus_areas: list[str],    ) -> list[dict[str, Any]]:        """Generate mock code review findings."""        findings = []        for file_path in files:            if "quality" in focus_areas:                findings.append(                    {                        "type": "code_quality",                        "severity": "medium",                        "file": file_path,                        "line": 42,                        "message": "Consider extracting complex logic into separate methods",                        "suggestion": "Break down large methods for better readability",                    }                )            if "bugs" in focus_areas:                findings.append(                    {                        "type": "potential_bug",                        "severity": "high",                        "file": file_path,                        "line": 85,                        "message": "Potential null pointer exception not handled",                        "suggestion": "Add null check before accessing object properties",                    }                )        return findings    def _generate_mock_dependency_analysis(self, files: list[str]) -> dict[str, Any]:        """Generate mock dependency analysis."""        return {            "modules": len(files),            "circular_dependencies": 0,            "coupling_score": 0.65,  # 0-1 scale            "high_coupling_pairs": [],        }    def _resolve_file_pattern(self, pattern: str) -> list[str]:        """Resolve glob pattern to list of actual files."""        try:            from glob import glob            files = glob(pattern, recursive=True)            return [f for f in files if f.endswith((".py", ".ts", ".js"))]        except Exception as e:            logger.warning(f"Failed to resolve file pattern {pattern}: {e}")            return []    def _save_analysis_artifacts(        self,        emit_paths: list[str],        analysis_data: dict[str, Any],    ) -> list[str]:        """Save analysis results to artifact files."""        artifacts = []        for emit_path in emit_paths:            try:                Path(emit_path).parent.mkdir(parents=True, exist_ok=True)                with open(emit_path, "w") as f:                    json.dump(analysis_data, f, indent=2)                artifacts.append(emit_path)            except Exception as e:                logger.warning(f"Failed to save analysis artifact {emit_path}: {e}")        return artifacts    def _estimate_tokens_for_analysis(self, files: list[str], detail_level: str) -> int:        """Estimate tokens needed for analysis."""        base_tokens = 1000  # Base analysis cost        # Estimate tokens per file        tokens_per_file = {            "low": 200,            "medium": 500,            "high": 1000,        }        file_tokens = len(files) * tokens_per_file.get(detail_level, 500)        return base_tokens + file_tokens    def _get_timestamp(self) -> str:        """Get current timestamp in ISO format."""        from datetime import datetime        return datetime.now().isoformat()    def validate_step(self, step: dict[str, Any]) -> bool:        """Validate that this adapter can execute the given step."""        with_params = self._extract_with_params(step)        # Check supported analysis types        analysis_type = with_params.get("analysis_type", "code_review")        supported_types = [            "code_review",            "architecture_analysis",            "refactor_planning",            "test_planning",        ]        return analysis_type in supported_types    def estimate_cost(self, step: dict[str, Any]) -> int:        """Estimate token cost for AI analysis operation."""        with_params = self._extract_with_params(step)        analysis_type = with_params.get("analysis_type", "code_review")        detail_level = with_params.get("detail_level", "medium")        # Base costs by analysis type        base_costs = {            "code_review": 2000,            "architecture_analysis": 3000,            "refactor_planning": 4000,            "test_planning": 2500,        }        # Detail level multipliers        detail_multipliers = {            "low": 0.7,            "medium": 1.0,            "high": 1.5,        }        base_cost = base_costs.get(analysis_type, 2000)        multiplier = detail_multipliers.get(detail_level, 1.0)        return int(base_cost * multiplier)    def is_available(self) -> bool:        """Check if AI analysis capabilities are available."""        # This adapter uses mock analysis for now        # In production, would check for AI API credentials        return True    def get_supported_analysis_types(self) -> list[str]:        """Get list of supported analysis types."""        return [            "code_review",            "architecture_analysis",            "refactor_planning",            "test_planning",        ]    def get_supported_focus_areas(self) -> list[str]:        """Get list of supported focus areas for analysis."""        return [            "quality",            "bugs",            "performance",            "security",            "maintainability",            "testability",            "documentation",        ]